#!/bin/bash

set -e

if [[ $# -eq 0 ]]; then
  echo Usage: $0 site_name [level_depth]
  echo EXAMPLE: $0 http://example/test.html 2
  exit 1;
else
  SITE=$1
  LEVEL=3
  if [[ $# -eq 2 ]]; then
    LEVEL=$2
  fi;
fi;

echo Site: $SITE
echo Depth: $LEVEL
wget -r -k -l $LEVEL -p -E -nc $SITE
exit 0;


#Скачиваем весь сайт
#Сформируем команду для загрузки каталога dir сайта site.ru (можно указывать и просто site.ru, я добавил каталог для наглядности возможностей wget):
#
#wget -r -l 6 -np -p -E -k -nc --restrict-file-names=windows -D site.ru https://site.ru/dir
#-r Включить рекурсивную загрузку.
#-D Список доменов, с которых разрешено загружать файлы. Чтобы wget не уходил по внешним ссылкам.
#-np Не подниматься выше начального адреса при рекурсивной загрузке.
#-p Загружать все файлы, которые нужны для отображения страниц HTML. Например: рисунки, звук, css.
#-E Если тип загруженного файла text/html и его адрес не оканчивается на .html, при использовании данного параметра к его имени будет добавлено .html.
#-k После завершения загрузки конвертировать ссылки в документе для просмотра в автономном режиме.
#—restrict-file-names экранирование символов, которых не может быть в имени файла. Чтобы сайт работал и в windows.
#-nc Для продолжения загрузки с места прерывания связи, если она оборвется.
#-l определяет максимальную глубину вложенности страниц, которые wget должен скачать
#Если wget скачивает только первую страницу — проверь, не указан ли параметр -D вместе с протоколом. То есть для данного параметра правильно указывать только домен — -D site.ru, а не домен с протоколом — -D http://site.ru.
